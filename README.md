# ğŸ—£ï¸ Accent-Agnostic Speech Transcription Model for Enhanced Accessibility

This project addresses the challenge of accent variability in speech recognition, particularly within e-learning environments, by developing a robust, accent-agnostic speech transcription system using deep learning techniques. Leveraging **MFCCs** for feature extraction and comparing **RNN**, **LSTM**, and **GRU** architectures, the **GRU-based model** demonstrated superior performance, making a meaningful step toward **inclusive and accessible education**.

---

## ğŸ§  Project Overview

- **ğŸ¯ Objective**: Build a speech recognition model that performs accurately across diverse English accents to improve transcription reliability in digital learning environments.
- **ğŸ“¦ Dataset**: [Mozilla Common Voice](https://commonvoice.mozilla.org/en/datasets) â€“ multilingual dataset rich in diverse accents.

---

## ğŸ› ï¸ Technologies Used

| Category            | Tools / Methods                           |
|---------------------|--------------------------------------------|
| Feature Extraction  | MFCCs (LibROSA)                            |
| Deep Learning       | RNN, LSTM, GRU (Keras/TensorFlow)          |
| Data Handling       | NumPy, Pandas                              |
| Visualization       | Matplotlib, Seaborn                        |
| Audio Conversion    | FFmpeg                                     |

---

## ğŸ”„ Methodology

### ğŸ” Data Preprocessing
- MP3 â†’ WAV conversion using **FFmpeg**
- Resampling to 16kHz
- Feature extraction using **MFCCs**
- Data normalization for convergence

### ğŸ§  Model Development
- Built and compared **RNN**, **LSTM**, and **GRU** models
- Applied **Dropout** and `TimeDistributed` dense layers
- Used **Categorical Crossentropy** loss and **Adam** optimizer

### ğŸ“Š Evaluation Metrics
- Accuracy
- Training/Validation Loss
- Generalization across accents

---

## ğŸ“ˆ Performance Summary

| Model | Training Accuracy | Validation Accuracy | Training Loss | Validation Loss |
|-------|-------------------|---------------------|----------------|-----------------|
| RNN   | 75.2%             | 72.8%               | 0.88           | 0.93            |
| LSTM  | 81.4%             | 79.5%               | 0.65           | 0.70            |
| GRU   | **83.9%**         | **82.3%**           | **0.59**       | **0.62**        |

âœ… **GRU** outperformed all other models in accuracy and generalization, proving most suitable for accent-agnostic transcription.

---

## ğŸ’¡ Key Findings

- GRU provided the best trade-off between **accuracy** and **computational efficiency**
- LSTM showed strong results but was more resource-heavy
- RNN struggled with long-term dependencies and overfitting

---

## ğŸ§© Applications

- ğŸ“ **E-learning**: Enhanced subtitle generation and real-time voice recognition
- ğŸ§ **Accessibility**: Assistive tools for hearing-impaired users or non-native speakers
- ğŸ’¼ **Voice-activated services**: For call centers, smart assistants, and multilingual platforms

---

## ğŸ‘¥ Team

**Team Name**: Titans Alliance  
- Ruchitha Paccha  
- Bojanapally Santhoshini  
- RamyaSri Muthineni

---

## ğŸ“š References

- Mozilla Common Voice Dataset  
- Goodfellow, Bengio & Courville â€“ *Deep Learning*  
- Graves et al. â€“ *Connectionist Temporal Classification*  
- Rabiner â€“ *Hidden Markov Models in Speech Recognition*  
- FFmpeg Documentation

---

## ğŸ“¬ Contact

For questions or collaborations, feel free to reach out:

ğŸ“§ **Email**: ruchitha1904@gmail.com  
ğŸ”— **LinkedIn**: [Ruchitha Paccha](https://www.linkedin.com/in/ruchitha-chowdary-paccha) 

